{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"b5c0d2440b3e4995a794ded565213150","deepnote_cell_type":"markdown"},"source":["<h1><center>Laboratorio 9: Optimizaci√≥n de modelos üíØ</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos</strong></center>"]},{"cell_type":"markdown","metadata":{"cell_id":"bfb94b9656f145ad83e81b75d218cb70","deepnote_cell_type":"markdown"},"source":["### Cuerpo Docente:\n","\n","- Profesor: Ignacio Meza, Gabriel Iturra\n","- Auxiliar: Sebasti√°n Tinoco\n","- Ayudante: Arturo Lazcano, Angelo Mu√±oz"]},{"cell_type":"markdown","metadata":{"cell_id":"b1b537fdd27c43909a49d3476ce64d91","deepnote_cell_type":"markdown"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n","\n","- Nombre de alumno 1: Nicol√°s Becerra\n","- Nombre de alumno 2: Sim√≥n Sanfeli√∫\n"]},{"cell_type":"markdown","metadata":{"cell_id":"b7dbdd30ab544cb8a8afe00648a586ae","deepnote_cell_type":"markdown"},"source":["## Temas a tratar\n","\n","- Predicci√≥n de demanda usando `xgboost`\n","- B√∫squeda del modelo √≥ptimo de clasificaci√≥n usando `optuna`\n","- Uso de pipelines.\n","\n","## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n","- Prohibidas las copias. \n","- Pueden usar cualquer material del curso que estimen conveniente.\n","\n","### Objetivos principales del laboratorio\n","\n","- Optimizar modelos usando `optuna`\n","- Recurrir a t√©cnicas de *prunning*\n","- Forzar el aprendizaje de relaciones entre variables mediante *constraints*\n","- Fijar un pipeline con un modelo base que luego se ir√° optimizando.\n","\n","El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."]},{"cell_type":"markdown","metadata":{"cell_id":"f38c8342f5164aa992a97488dd5590bf","deepnote_cell_type":"markdown"},"source":["### **Link de repositorio de GitHub:** https://github.com/SimonSanfeliu/MDS7202-BS/tree/L9"]},{"cell_type":"markdown","metadata":{"cell_id":"f1c73babb7f74af588a4fa6ae14829e0","deepnote_cell_type":"markdown"},"source":["# Importamos librerias √∫tiles"]},{"cell_type":"code","execution_count":47,"metadata":{"cell_id":"51afe4d2df42442b9e5402ffece60ead","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4957,"execution_start":1699544354044,"source_hash":null},"outputs":[],"source":["!pip install -qq xgboost optuna"]},{"cell_type":"markdown","metadata":{"cell_id":"44d227389a734ac59189c5e0005bc68a","deepnote_cell_type":"markdown"},"source":["# 1. El emprendimiento de Fiu\n","\n","Tras liderar de manera exitosa la implementaci√≥n de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corp√≥reo **Fiu** se anima y decide levantar su propio negocio de consultor√≠a en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Como usted tuvo un rendimiento sobresaliente en el proyecto de caracterizaci√≥n de datos, Fiu lo contrata como *data scientist* de su emprendimiento.\n","\n","Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n","\n","Para comenzar, cargue el dataset se√±alado y visualice a trav√©s de un `.head` los atributos que posee el dataset.\n","\n","<i><p align=\"center\">Fiu siendo felicitado por su excelente desempe√±o en el proyecto de caracterizaci√≥n de datos</p></i>\n","<p align=\"center\">\n","  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n","</p>"]},{"cell_type":"code","execution_count":48,"metadata":{"cell_id":"2f9c82d204b14515ad27ae07e0b77702","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":92,"execution_start":1699544359006,"source_hash":null},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\sanfe\\AppData\\Local\\Temp\\ipykernel_20376\\3184305967.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df['date'] = pd.to_datetime(df['date'])\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>city</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>pop</th>\n","      <th>shop</th>\n","      <th>brand</th>\n","      <th>container</th>\n","      <th>capacity</th>\n","      <th>price</th>\n","      <th>quantity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>0.96</td>\n","      <td>13280</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>2.86</td>\n","      <td>6727</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.87</td>\n","      <td>9848</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>adult-cola</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>1.00</td>\n","      <td>20050</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>adult-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.39</td>\n","      <td>25696</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id       date    city       lat      long     pop    shop        brand  \\\n","0   0 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","1   1 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","2   2 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","3   3 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n","4   4 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n","\n","  container capacity  price  quantity  \n","0     glass    500ml   0.96     13280  \n","1   plastic    1.5lt   2.86      6727  \n","2       can    330ml   0.87      9848  \n","3     glass    500ml   1.00     20050  \n","4       can    330ml   0.39     25696  "]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","df = pd.read_csv('sales.csv')\n","df['date'] = pd.to_datetime(df['date'])\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"b50db6f2cb804932ae3f9e5748a6ea61","deepnote_cell_type":"markdown"},"source":["## 1.1 Generando un Baseline (0.5 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n","</p>\n","\n","Antes de entrenar un algoritmo, usted recuerda los apuntes de su mag√≠ster en ciencia de datos y recuerda que debe seguir una serie de *buenas pr√°cticas* para entrenar correcta y debidamente su modelo. Despu√©s de un par de vueltas, llega a las siguientes tareas:\n","\n","1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad.\n","2. Implemente un `FunctionTransformer` para extraer el d√≠a, mes y a√±o de la variable `date`. Guarde estas variables en el formato categorical de pandas.\n","3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos num√©ricos y categ√≥ricos. Use `OneHotEncoder` para las variables categ√≥ricas.\n","4. Guarde los pasos anteriores en un `Pipeline`, dejando como √∫ltimo paso el regresor `DummyRegressor` para generar predicciones en base a promedios.\n","5. Entrene el pipeline anterior y reporte la m√©trica `mean_absolute_error` sobre los datos de validaci√≥n. ¬øC√≥mo se interpreta esta m√©trica para el contexto del negocio?\n","6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los par√°metros por default**. ¬øC√≥mo cambia el MAE al implementar este algoritmo? ¬øEs mejor o peor que el `DummyRegressor`?\n","7. Guarde ambos modelos en un archivo .pkl (uno cada uno)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# Funci√≥n para FunctionTransformer\n","def extract_date(df):\n","    \"\"\"\n","    Agregar docstring \n","    \"\"\"\n","    # Revisando datos\n","    assert type(df) == pd.DataFrame\n","    assert \"date\" in df.columns\n","\n","    # Generando las columnas\n","    df = df.assign(\n","        year = [d.year for d in df[\"date\"].dt.date],\n","        month = [d.month for d in df[\"date\"].dt.date],\n","        day = [d.day for d in df[\"date\"].dt.date]\n","    )\n","\n","    # Transform√°ndolas en categor√≠as\n","    df = df.astype(\n","        {\n","            \"day\": \"category\",\n","            \"month\": \"category\",\n","            \"year\": \"category\"\n","        }\n","    )\n","\n","    return df\n","\n","# Funci√≥n para transformaci√≥n logar√≠tmica\n","def to_log(df_s):\n","    \"\"\"\n","    Agregar docstring \n","    \"\"\"\n","    # Revisando datos\n","    assert type(df_s) == pd.DataFrame\n","\n","    # Transformando los datos de la serie a escala logar√≠tmica\n","    df_s = df_s.apply(lambda x: np.log(x + 1))\n","    return df_s"]},{"cell_type":"code","execution_count":84,"metadata":{"cell_id":"1482c992d9494e5582b23dbd3431dbfd","deepnote_cell_type":"code"},"outputs":[],"source":["# Obteniendo librer√≠as necesarias\n","from sklearn.model_selection import train_test_split \n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.dummy import DummyRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","# Definiendo la semilla\n","RANDOM_STATE = 42\n","\n","# Separando el conjunto de datos\n","X_train, X_rest, y_train, y_rest = train_test_split(df, df[\"quantity\"].to_frame(), test_size=.3, random_state=RANDOM_STATE)\n","X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=.33, random_state=RANDOM_STATE)\n","X_train.drop(columns=\"quantity\", inplace=True)\n","X_val.drop(columns=\"quantity\", inplace=True)\n","X_test.drop(columns=\"quantity\", inplace=True)\n","\n","# Separando los datos en num√©ricos y categ√≥ricos\n","num_cols = df.corr(numeric_only=True).columns.to_list()\n","num_cols.remove(\"quantity\")\n","cat_cols = [col for col in df.columns if not col in num_cols]\n","cat_cols.remove(\"quantity\")\n","cat_cols.append(\"year\")  # Agregando las nuevas columnas que se obtendr√°n del FunctionTransformer\n","cat_cols.append(\"month\")\n","cat_cols.append(\"day\")\n","\n","# Atributos num√©ricos\n","num_pipe = Pipeline([\n","                ('Logaritmic scaler', FunctionTransformer(to_log, feature_names_out='one-to-one')),\n","                ('MinMax scaler', MinMaxScaler())\n","            ])\n","# Atributos categ√≥ricos\n","cat_pipe = Pipeline([\n","    ('Encoder', OneHotEncoder())\n","])\n","\n","# Creando ColumnTransformer\n","ctrans = ColumnTransformer(\n","        transformers=[\n","            (\"Categorico\", cat_pipe, cat_cols),\n","            (\"Numerico\", num_pipe, num_cols),\n","        ]\n",")"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE sobre conjunto de validaci√≥n: 13308.134750658153\n"]}],"source":["# Pipeline de entrenamiento 1\n","pipe_train1 = Pipeline([\n","    (\"Date extraction\", FunctionTransformer(extract_date)),\n","    (\"Scaling\", ctrans),\n","    (\"Classifier\", DummyRegressor())\n","])\n","\n","# Entrenando\n","model1 = pipe_train1.fit(X_train, y_train)\n","\n","# Prediciendo\n","y_pred1 = model1.predict(X_val)\n","\n","# MAE\n","mae1 = mean_absolute_error(y_val, y_pred1)\n","print(f\"MAE sobre conjunto de validaci√≥n: {mae1}\")"]},{"cell_type":"markdown","metadata":{},"source":["> El `mean_absolute_error` es una medida de diferencia entre el valor predicho y el valor real, siendo √©sta el error absoluto medio entre ambos. Lo ideal es que esta m√©trica sea lo m√°s cercana a 0 posible, ya que implicar√≠a que el valor predicho no dista mucho del real, por lo que se tendr√≠a una buena predicci√≥n. \n","\n","> Dado que el valor obtenido del MAE es alrededor de 13000, se tiene que para la pipeline generada con `DummyRegressor` es de muy baja calidad, ya que los valores predichos est√°n muy alejados de los valores reales. As√≠, se tiene que el primer modelo generado es muy malo para la predicci√≥n de la demanda de cantidad de ventas, por lo que la m√©trica estar√≠a diciendo que este modelo no es bueno para el negocio."]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE sobre conjunto de validaci√≥n: 2524.5526449002314\n"]}],"source":["import xgboost as xgb\n","\n","# Pipeline de entrenamiento 2\n","pipe_train2 = Pipeline([\n","    (\"Date extraction\", FunctionTransformer(extract_date)),\n","    (\"Scaling\", ctrans),\n","    (\"Classifier\", xgb.XGBRegressor())\n","])\n","\n","# Entrenando\n","model2 = pipe_train2.fit(X_train, y_train)\n","\n","# Prediciendo\n","y_pred2 = model2.predict(X_val)\n","\n","# MAE\n","mae2 = mean_absolute_error(y_val, y_pred2)\n","print(f\"MAE sobre conjunto de validaci√≥n: {mae2}\")"]},{"cell_type":"markdown","metadata":{},"source":["> Al cambiar el `DummyRegressor` por el `XGBRegressor`, se tiene una mejora instant√°nea en la m√©trica, teniendo ahora un MAE de cerca de 2500, lo que es mucho m√°s cercano a 0 que 13000. Este valor sigue indicando que el clasificador es malo para el negocio, pero muestra una inmediata mejora con el anterior."]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["['models/default_xgb_model.pkl']"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import joblib\n","\n","# Creando la carpeta de los modelos si es que no existe\n","if not os.path.exists(\"models\"):\n","    os.mkdir(\"models\")\n","\n","# Guardando los archivos en pkls separados\n","joblib.dump(model1, \"models/dummy_model.pkl\")\n","joblib.dump(model2, \"models/default_xgb_model.pkl\")"]},{"cell_type":"markdown","metadata":{"cell_id":"7e17e46063774ec28226fe300d42ffe0","deepnote_cell_type":"markdown"},"source":["## 1.2 Forzando relaciones entre par√°metros con XGBoost (1.0 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n","</p>\n","\n","Un colega aficionado a la econom√≠a le *sopla* que la demanda guarda una relaci√≥n inversa con el precio del producto. Motivado para impresionar al querido corp√≥reo, se propone hacer uso de esta informaci√≥n para mejorar su modelo.\n","\n","Vuelva a entrenar el `Pipeline`, pero esta vez forzando una relaci√≥n mon√≥tona negativa entre el precio y la cantidad. Luego, vuelva a reportar el `MAE` sobre el conjunto de validaci√≥n. ¬øC√≥mo cambia el error al incluir esta relaci√≥n? ¬øTen√≠a raz√≥n su amigo?\n","\n","Nuevamente, guarde su modelo en un archivo .pkl\n","\n","Nota: Para realizar esta parte, debe apoyarse en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentaci√≥n</a>.\n","\n","Hint: Para implementar el constraint, se le sugiere hacerlo especificando el nombre de la variable. De ser as√≠, probablemente le sea √∫til **mantener el formato de pandas** antes del step de entrenamiento."]},{"cell_type":"code","execution_count":83,"metadata":{"cell_id":"f469f3b572be434191d2d5c3f11b20d2","deepnote_cell_type":"code"},"outputs":[{"ename":"ValueError","evalue":"Constrained features are not a subset of training data feature names","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\sanfe\\Documents\\M√≠o\\Trabajos\\Universidad\\El√©ctrica\\Semestre 7\\Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\MDS7202-BS\\L9\\Lab9_Becerra_Sanfeli√∫.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pipe_train3 \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mDate extraction\u001b[39m\u001b[39m\"\u001b[39m, FunctionTransformer(extract_date)),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mScaling\u001b[39m\u001b[39m\"\u001b[39m, ctrans),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m\"\u001b[39m, xgb\u001b[39m.\u001b[39mXGBRegressor(monotone_constraints\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m}))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Entrenando\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model3 \u001b[39m=\u001b[39m pipe_train3\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Prediciendo\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/L9/Lab9_Becerra_Sanfeli%C3%BA.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m y_pred3 \u001b[39m=\u001b[39m model3\u001b[39m.\u001b[39mpredict(X_val)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m (\n\u001b[0;32m   1078\u001b[0m     model,\n\u001b[0;32m   1079\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1085\u001b[0m )\n\u001b[1;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1087\u001b[0m     params,\n\u001b[0;32m   1088\u001b[0m     train_dmatrix,\n\u001b[0;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1098\u001b[0m )\n\u001b[0;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:159\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    156\u001b[0m metric_fn \u001b[39m=\u001b[39m _configure_custom_metric(feval, custom_metric)\n\u001b[0;32m    157\u001b[0m evals \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(evals) \u001b[39mif\u001b[39;00m evals \u001b[39melse\u001b[39;00m []\n\u001b[1;32m--> 159\u001b[0m bst \u001b[39m=\u001b[39m Booster(params, [dtrain] \u001b[39m+\u001b[39;49m [d[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m evals], model_file\u001b[39m=\u001b[39;49mxgb_model)\n\u001b[0;32m    160\u001b[0m start_iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m verbose_eval:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:1687\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, cache, model_file)\u001b[0m\n\u001b[0;32m   1685\u001b[0m params \u001b[39m=\u001b[39m params \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m   1686\u001b[0m params_processed \u001b[39m=\u001b[39m _configure_metrics(params\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m-> 1687\u001b[0m params_processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_configure_constraints(params_processed)\n\u001b[0;32m   1688\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(params_processed, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   1689\u001b[0m     params_processed\u001b[39m.\u001b[39mappend((\u001b[39m\"\u001b[39m\u001b[39mvalidate_parameters\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m))\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:1744\u001b[0m, in \u001b[0;36mBooster._configure_constraints\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m   1741\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmonotone_constraints\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1744\u001b[0m     params[idx] \u001b[39m=\u001b[39m (name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_monotone_constrains(value))\n\u001b[0;32m   1745\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minteraction_constraints\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1746\u001b[0m     params[idx] \u001b[39m=\u001b[39m (name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_interaction_constraints(value))\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:1706\u001b[0m, in \u001b[0;36mBooster._transform_monotone_constrains\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   1704\u001b[0m feature_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names \u001b[39mor\u001b[39;00m []\n\u001b[0;32m   1705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m constrained_features\u001b[39m.\u001b[39missubset(\u001b[39mset\u001b[39m(feature_names)):\n\u001b[1;32m-> 1706\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1707\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConstrained features are not a subset of training data feature names\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1708\u001b[0m     )\n\u001b[0;32m   1710\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mget(name, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m feature_names)\n","\u001b[1;31mValueError\u001b[0m: Constrained features are not a subset of training data feature names"]}],"source":["# Pipeline de entrenamiento 3\n","pipe_train3 = Pipeline([\n","    (\"Date extraction\", FunctionTransformer(extract_date)),\n","    (\"Scaling\", ctrans),\n","    (\"Classifier\", xgb.XGBRegressor(monotone_constraints={\"price\": -1}))\n","])\n","\n","# Entrenando\n","model3 = pipe_train3.fit(X_train, y_train)\n","\n","# Prediciendo\n","y_pred3 = model3.predict(X_val)\n","\n","# MAE\n","mae3 = mean_absolute_error(y_val, y_pred3)\n","print(f\"MAE sobre conjunto de validaci√≥n: {mae3}\")"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>city</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>pop</th>\n","      <th>shop</th>\n","      <th>brand</th>\n","      <th>container</th>\n","      <th>capacity</th>\n","      <th>price</th>\n","      <th>quantity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>292</th>\n","      <td>300</td>\n","      <td>2012-04-30</td>\n","      <td>Patra</td>\n","      <td>38.24444</td>\n","      <td>21.73444</td>\n","      <td>164250</td>\n","      <td>shop_6</td>\n","      <td>adult-cola</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>2.54</td>\n","      <td>27447</td>\n","    </tr>\n","    <tr>\n","      <th>3366</th>\n","      <td>3416</td>\n","      <td>2015-02-28</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>667237</td>\n","      <td>shop_1</td>\n","      <td>gazoza</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>0.71</td>\n","      <td>17738</td>\n","    </tr>\n","    <tr>\n","      <th>3685</th>\n","      <td>3741</td>\n","      <td>2015-06-30</td>\n","      <td>Athens</td>\n","      <td>37.96245</td>\n","      <td>23.68708</td>\n","      <td>667237</td>\n","      <td>shop_3</td>\n","      <td>adult-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.66</td>\n","      <td>31618</td>\n","    </tr>\n","    <tr>\n","      <th>2404</th>\n","      <td>2441</td>\n","      <td>2014-04-30</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>668203</td>\n","      <td>shop_1</td>\n","      <td>gazoza</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.30</td>\n","      <td>50015</td>\n","    </tr>\n","    <tr>\n","      <th>2855</th>\n","      <td>2898</td>\n","      <td>2014-09-30</td>\n","      <td>Irakleion</td>\n","      <td>35.32787</td>\n","      <td>25.14341</td>\n","      <td>136202</td>\n","      <td>shop_2</td>\n","      <td>orange-power</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.56</td>\n","      <td>36756</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5191</th>\n","      <td>5275</td>\n","      <td>2016-11-30</td>\n","      <td>Athens</td>\n","      <td>37.96245</td>\n","      <td>23.68708</td>\n","      <td>665102</td>\n","      <td>shop_3</td>\n","      <td>adult-cola</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>3.59</td>\n","      <td>7389</td>\n","    </tr>\n","    <tr>\n","      <th>5226</th>\n","      <td>5311</td>\n","      <td>2016-12-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>665102</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>2.78</td>\n","      <td>12388</td>\n","    </tr>\n","    <tr>\n","      <th>5390</th>\n","      <td>5478</td>\n","      <td>2017-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>665871</td>\n","      <td>shop_1</td>\n","      <td>gazoza</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.36</td>\n","      <td>27956</td>\n","    </tr>\n","    <tr>\n","      <th>860</th>\n","      <td>873</td>\n","      <td>2012-10-31</td>\n","      <td>Athens</td>\n","      <td>37.96245</td>\n","      <td>23.68708</td>\n","      <td>672130</td>\n","      <td>shop_3</td>\n","      <td>gazoza</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>0.51</td>\n","      <td>32603</td>\n","    </tr>\n","    <tr>\n","      <th>7270</th>\n","      <td>7374</td>\n","      <td>2018-10-31</td>\n","      <td>Athens</td>\n","      <td>37.96245</td>\n","      <td>23.68708</td>\n","      <td>664046</td>\n","      <td>shop_3</td>\n","      <td>adult-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.81</td>\n","      <td>16908</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5219 rows √ó 12 columns</p>\n","</div>"],"text/plain":["        id       date       city       lat      long     pop    shop  \\\n","292    300 2012-04-30      Patra  38.24444  21.73444  164250  shop_6   \n","3366  3416 2015-02-28     Athens  37.97945  23.71622  667237  shop_1   \n","3685  3741 2015-06-30     Athens  37.96245  23.68708  667237  shop_3   \n","2404  2441 2014-04-30     Athens  37.97945  23.71622  668203  shop_1   \n","2855  2898 2014-09-30  Irakleion  35.32787  25.14341  136202  shop_2   \n","...    ...        ...        ...       ...       ...     ...     ...   \n","5191  5275 2016-11-30     Athens  37.96245  23.68708  665102  shop_3   \n","5226  5311 2016-12-31     Athens  37.97945  23.71622  665102  shop_1   \n","5390  5478 2017-01-31     Athens  37.97945  23.71622  665871  shop_1   \n","860    873 2012-10-31     Athens  37.96245  23.68708  672130  shop_3   \n","7270  7374 2018-10-31     Athens  37.96245  23.68708  664046  shop_3   \n","\n","             brand container capacity  price  quantity  \n","292     adult-cola   plastic    1.5lt   2.54     27447  \n","3366        gazoza   plastic    1.5lt   0.71     17738  \n","3685    adult-cola       can    330ml   0.66     31618  \n","2404        gazoza       can    330ml   0.30     50015  \n","2855  orange-power       can    330ml   0.56     36756  \n","...            ...       ...      ...    ...       ...  \n","5191    adult-cola   plastic    1.5lt   3.59      7389  \n","5226   kinder-cola   plastic    1.5lt   2.78     12388  \n","5390        gazoza       can    330ml   0.36     27956  \n","860         gazoza     glass    500ml   0.51     32603  \n","7270    adult-cola       can    330ml   0.81     16908  \n","\n","[5219 rows x 12 columns]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"markdown","metadata":{},"source":["> Explicaci√≥n XD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Guardando el modelo\n","joblib.dump(model3, \"models/constraint_xgb_model.pkl\")"]},{"cell_type":"markdown","metadata":{"cell_id":"e59ef80ed20b4de8921f24da74e87374","deepnote_cell_type":"markdown"},"source":["## 1.3 Optimizaci√≥n de Hiperpar√°metros con Optuna (2.0 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n","</p>\n","\n","Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun m√°s* su modelo. En particular, le comenta de la optimizaci√≥n de hiperpar√°metros con metodolog√≠as bayesianas a trav√©s del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n","\n","A partir de la mejor configuraci√≥n obtenida en la secci√≥n anterior, utilice `optuna` para optimizar sus hiperpar√°metros. En particular, se le pide:\n","\n","- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n","- Utilice `TPESampler` como m√©todo de muestreo\n","- De `XGBRegressor`, optimice los siguientes hiperpar√°metros:\n","    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n","    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n","    - `max_depth` buscando valores enteros en el rango (3, 10)\n","    - `max_leaves` buscando valores enteros en el rango (0, 100)\n","    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n","    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n","    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n","- De `OneHotEncoder`, optimice el hiperpar√°metro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n","- Explique cada hiperpar√°metro y su rol en el modelo. ¬øHacen sentido los rangos de optimizaci√≥n indicados?\n","- Fije el tiempo de entrenamiento a 5 minutos\n","- Reportar el n√∫mero de *trials*, el `MAE` y los mejores hiperpar√°metros encontrados. ¬øC√≥mo cambian sus resultados con respecto a la secci√≥n anterior? ¬øA qu√© se puede deber esto?\n","- Guardar su modelo en un archivo .pkl"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"de5914621cc64cb0b1bacb9ff565a97e","deepnote_cell_type":"code"},"outputs":[],"source":["# Inserte su c√≥digo ac√°"]},{"cell_type":"markdown","metadata":{"cell_id":"5195ccfc37e044ad9453f6eb2754f631","deepnote_cell_type":"markdown"},"source":["## 1.4 Optimizaci√≥n de Hiperpar√°metros con Optuna y Prunners (1.7)\n","\n","<p align=\"center\">\n","  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n","</p>\n","\n","Despu√©s de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en s√≠ mismo. Despu√©s de leer un par de post de personas de dudosa reputaci√≥n en la *deepweb*, usted llega a la conclusi√≥n que puede cumplir este objetivo mediante la implementaci√≥n de **Prunning**.\n","\n","Vuelva a optimizar los mismos hiperpar√°metros que la secci√≥n pasada, pero esta vez utilizando **Prunning** en la optimizaci√≥n. En particular, usted debe:\n","\n","- Responder: ¬øQu√© es prunning? ¬øDe qu√© forma deber√≠a impactar en el entrenamiento?\n","- Utilizar `optuna.integration.XGBoostPruningCallback` como m√©todo de **Prunning**\n","- Fijar nuevamente el tiempo de entrenamiento a 5 minutos\n","- Reportar el n√∫mero de *trials*, el `MAE` y los mejores hiperpar√°metros encontrados. ¬øC√≥mo cambian sus resultados con respecto a la secci√≥n anterior? ¬øA qu√© se puede deber esto?\n","- Guardar su modelo en un archivo .pkl\n","\n","Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n","\n","```\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","```\n","\n","De implementar la opci√≥n anterior, pueden especificar `show_progress_bar = True` en el m√©todo `optimize` para *m√°s sabor*.\n","\n","Hint: Si quieren especificar par√°metros del m√©todo .fit() del modelo a trav√©s del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n","\n","Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementaci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"eeaa967cd8f6426d8c54f276c17dce79","deepnote_cell_type":"code"},"outputs":[],"source":["# Inserte su c√≥digo ac√°"]},{"cell_type":"markdown","metadata":{"cell_id":"8a081778cc704fc6bed05393a5419327","deepnote_cell_type":"markdown"},"source":["## 1.5 Visualizaciones (0.5 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n","</p>\n","\n","\n","Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n","\n","A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n","\n","- Gr√°fico de historial de optimizaci√≥n\n","- Gr√°fico de coordenadas paralelas\n","- Gr√°fico de importancia de hiperpar√°metros\n","\n","Comente sus resultados: ¬øDesde qu√© *trial* se empiezan a observar mejoras notables en sus resultados? ¬øQu√© tendencias puede observar a partir del gr√°fico de coordenadas paralelas? ¬øCu√°les son los hiperpar√°metros con mayor importancia para la optimizaci√≥n de su modelo?"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"0e706dc9a8d946eda7a9eb1f0463c6d7","deepnote_cell_type":"code"},"outputs":[],"source":["# Inserte su c√≥digo ac√°"]},{"cell_type":"markdown","metadata":{"cell_id":"ac8a20f445d045a3becf1a518d410a7d","deepnote_cell_type":"markdown"},"source":["## 1.6 S√≠ntesis de resultados (0.3)\n","\n","Finalmente, genere una tabla resumen del MAE obtenido en los 5 modelos entrenados (desde Baseline hasta XGBoost con Constraints, Optuna y Prunning) y compare sus resultados. ¬øQu√© modelo obtiene el mejor rendimiento? \n","\n","Por √∫ltimo, cargue el mejor modelo, prediga sobre el conjunto de test y reporte su MAE. ¬øExisten diferencias con respecto a las m√©tricas obtenidas en el conjunto de validaci√≥n? ¬øPorqu√© puede ocurrir esto?"]},{"cell_type":"markdown","metadata":{"cell_id":"5c4654d12037494fbd385b4dc6bd1059","deepnote_cell_type":"markdown"},"source":["# Conclusi√≥n\n","Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/8CT1AXElF_cAAAAC/gojo-satoru.gif\">\n","</p>"]},{"cell_type":"markdown","metadata":{"cell_id":"5025de06759f4903a26916c80323bf25","deepnote_cell_type":"markdown"},"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"f63d38450a6b464c9bb6385cf11db4d9","deepnote_persisted_session":{"createdAt":"2023-11-09T16:18:30.203Z"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
