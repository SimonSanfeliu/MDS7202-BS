{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Nicol√°s Becerra\n",
    "- Sim√≥n Sanfeli√∫\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: https://github.com/SimonSanfeliu/MDS7202-BS/tree/Proyecto-1\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as necesarias para el proyecto\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import sklearn\n",
    "\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Librer√°s para plotear\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funciones auxiliares √∫tiles\n",
    "\"\"\"\n",
    "\n",
    "# Funci√≥n para obtener la V de Cramer\n",
    "# Obtenida de https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n",
    "\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorical-categorical association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "#Funci√≥n para separar columnas\n",
    "def separate_columns(df, cols):\n",
    "    \"\"\"\n",
    "    Funci√≥n que separa columnas no at√≥micas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame con las columnas a separar\n",
    "    cols: list\n",
    "        Lista de columnas a separar\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_new: DataFrame\n",
    "        DataFrame con las columnas separadas\n",
    "    \"\"\"\n",
    "    assert isinstance(df, pd.DataFrame), 'df debe ser un DataFrame'\n",
    "    assert isinstance(cols, list), 'cols debe ser una lista'\n",
    "    df_new = df.copy()\n",
    "    for col in cols:\n",
    "        new_cols = col.split('-')\n",
    "        temp = np.vstack(df_new[col].apply(lambda x: re.findall(r'(\\d+[.]\\d+|\\w+|\\d+)', x)).to_numpy())\n",
    "        for i in range(temp.shape[1]):\n",
    "            df_new[new_cols[i].capitalize()] = temp[:, i].astype(float)\n",
    "        df_new.drop(columns=col, inplace=True)\n",
    "    return df_new, df_new.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler():\n",
    "    \"\"\"\n",
    "    Clase que permite hacer un an√°lisis exploratorio de datos (EDA) de un dataframe. \n",
    "    Se puede hacer un resumen de las variables, hacer gr√°ficos de las variables, limpiar\n",
    "    los datos, escalar los datos, hacer clusters y detectar anomal√≠as, junto con eliminar los registros de \n",
    "    cada m√©todo en caso de que se quiera.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame a analizar\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame a analizar\n",
    "    eda_path: str\n",
    "        Nombre de la carpeta donde se guardan los resultados del EDA\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    summarize(columns, anomalies)\n",
    "        Caracterizar las variables del dataframe. Se puede hacer de una variable o de varias.\n",
    "        Reporta informaci√≥n anal√≠tica y estad√≠stica de primera l√≠nea de las variables, y guarda\n",
    "        un archivo txt con lo obtenido en la carpeta EDA_fecha. En caso de anomal√≠as, se incluye en el reporte.\n",
    "    \n",
    "    plot_vars(columns, N)\n",
    "        Grafica la distribuci√≥n y correlaci√≥n de las variables. Se puede hacer de una variable o de varias.\n",
    "        Guarda los gr√°ficos en la carpeta EDA_fecha. En caso de que las variables sean categ√≥ricas, se hace\n",
    "        una matriz de correlaci√≥n de la V de Cramer. En caso de que sean num√©ricas, se hace una matriz de\n",
    "        correlaci√≥n de Pearson.\n",
    "\n",
    "    clean_data(columns, nonatomic, drop_na)\n",
    "        Limpia los datos. Se puede hacer de una variable o de varias. Se pueden eliminar los registros con\n",
    "        valores nulos o no. Guarda el dataframe limpio en la carpeta EDA_fecha.\n",
    "\n",
    "    scale(*args, columns, nonatomic, drop_na)\n",
    "        Escala los datos. Se puede hacer de una variable o de varias. Una vez limpio el dataframe, se \n",
    "        ingresan los m√©todos de escalamiento que se quieran aplicar, que sean compatibles con\n",
    "        sklearn. Guarda el dataframe escalado en la carpeta EDA_fecha.\n",
    "\n",
    "    make_clusters(clustering, k, threshold, *args, columns, nonatomic, drop_na)\n",
    "        Genera clusters de los datos usando alg√∫n m√©todo de clustering de sklearn. Se puede hacer de una variable o de varias.\n",
    "        El dataframe se limpia y escala autom√°ticamente. Guarda el dataframe con los clusters en la carpeta EDA_fecha.\n",
    "\n",
    "    detect_anomalies(detector, *args, columns, nonatomic, drop_na)\n",
    "        Detecta anomal√≠as en los datos usando alguna t√©cnica de detecci√≥n de anomal√≠as de sklearn. Se puede hacer de una variable o de varias.\n",
    "        El dataframe se limpia y escala autom√°ticamente. Guarda el dataframe con las anomal√≠as en la carpeta EDA_fecha.\n",
    "\n",
    "    profile(columns, anomalies, N, nonatomic, drop_na, clustering, k, threshold, detector, *args)\n",
    "        Funci√≥n que permite hacer un an√°lisis exploratorio de datos (EDA) de un dataframe, ejecutando todos los m√©todos.\n",
    "\n",
    "    clearGarbage()\n",
    "        Elimina la carpeta del d√≠a si es que existe. Si no existe, no hace nada.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Agregar docstring\n",
    "        \"\"\"\n",
    "        assert type(df) == pd.DataFrame\n",
    "        self.df = df\n",
    "        self.eda_path = f\"EDA_{datetime.now().strftime('%d-%m-%Y')}\"\n",
    "        if not os.path.exists(self.eda_path):\n",
    "            os.mkdir(self.eda_path)\n",
    "\n",
    "    def summarize(self, columns, anomalies):\n",
    "        \"\"\"\n",
    "        Caracterizar las variables del dataframe. Se puede hacer de una variable o de varias.\n",
    "        Reporta informaci√≥n anal√≠tica y estad√≠stica de primera l√≠nea de las variables, y guarda\n",
    "        un archivo txt con lo obtenido en la carpeta EDA_fecha. En caso de anomal√≠as, se incluye en el reporte.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a caracterizar\n",
    "        anomalies: list\n",
    "            Lista de anomal√≠as a revisar. Actualmente s√≥lo se puede revisar el formato de las variables\n",
    "        \"\"\"\n",
    "        # Viendo los tipos\n",
    "        assert type(columns) == str or type(columns) == list\n",
    "        assert type(anomalies) == list\n",
    "\n",
    "        if type(columns) == str:\n",
    "            serie_in = self.df[columns]\n",
    "            serie = serie_in.copy()\n",
    "\n",
    "            # Revisando anomal√≠as\n",
    "            if len(anomalies) != 0:\n",
    "                for a in anomalies:\n",
    "                    if a == \"format\" and not is_numeric_dtype(serie):\n",
    "                        chars = [\"*\", \"(\", \"?\", \":\"]\n",
    "                        check = serie.apply(lambda x: \"outlier\" if any(a in x for a in chars) else x)\n",
    "                        if \"outlier\" in check.values:\n",
    "                            raise Exception(f\"Anomal√≠a detectada en columna {columns}: caracteres extra√±os\")\n",
    "\n",
    "            profile = pd.Series(dtype='object')\n",
    "            profile[\"Name\"] = columns\n",
    "            profile[\"Type\"] = serie.dtype\n",
    "            profile = pd.concat([profile, serie.describe(percentiles=[.25, .5, .75, 1], datetime_is_numeric=True)])\n",
    "\n",
    "            if is_numeric_dtype(serie):\n",
    "                profile[\"Negative\"] = (serie < 0).sum()\n",
    "                profile[\"Negative (%)\"] = (\n",
    "                    str(round((serie < 0).sum() / len(serie) * 100, 2)) + \" %\"\n",
    "                )\n",
    "                profile[\"Zeros\"] = (serie == 0).sum()\n",
    "                profile[\"Zeros (%)\"] = (\n",
    "                    str(round((serie == 0).sum() / len(serie) * 100, 2)) + \" %\"\n",
    "                )\n",
    "                only_outliers = serie.loc[(np.abs(ss.zscore(serie)) >= 3)]\n",
    "                profile[\"Outliers\"] = only_outliers.shape[0]\n",
    "                profile[\"Outliers (%)\"] = (\n",
    "                    str(round(only_outliers.shape[0] / len(serie) * 100, 2)) + \" %\"\n",
    "                )\n",
    "\n",
    "            profile[\"Missing cells\"] = serie.isnull().sum()\n",
    "            profile[\"Missing cells (%)\"] = (\n",
    "                str(round(serie.isnull().sum() / len(serie) * 100, 2)) + \" %\"\n",
    "            )\n",
    "\n",
    "            profile = profile.rename(\n",
    "                index={\n",
    "                    \"count\": \"Number of observations\",\n",
    "                    \"mean\": \"Mean\",\n",
    "                    \"std\": \"Std\",\n",
    "                    \"min\": \"Min\",\n",
    "                    \"max\": \"Max\",\n",
    "                    \"unique\": \"Unique\",\n",
    "                    \"top\": \"Top\",\n",
    "                    \"freq\": \"Freq\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Pasando la data a un txt\n",
    "            txt = \"Reporte de variable \\n\\n\" + profile.to_string()\n",
    "            new_file = open(f\"{self.eda_path}/summary.txt\", \"w+\")\n",
    "            new_file.write(txt)\n",
    "            new_file.close()\n",
    "\n",
    "        else:\n",
    "            txt = \"Reporte de variables \\n\\n\"\n",
    "            for c in columns:\n",
    "                serie_in = self.df[c]\n",
    "                serie = serie_in.copy()\n",
    "\n",
    "                # Revisando anomal√≠as\n",
    "                if len(anomalies) != 0:\n",
    "                    for a in anomalies:\n",
    "                        if a == \"format\" and not is_numeric_dtype(serie):\n",
    "                            chars = [\"*\", \"(\", \"?\", \":\"]\n",
    "                            check = serie.apply(lambda x: \"outlier\" if any(a in x for a in chars) else x)\n",
    "                            if \"outlier\" in check.values:\n",
    "                                raise Exception(f\"Anomal√≠a detectada en columna {c}: caracteres extra√±os\")\n",
    "                        \n",
    "\n",
    "                profile = pd.Series(dtype='object')\n",
    "                profile[\"Name\"] = c\n",
    "                profile[\"Type\"] = serie.dtype\n",
    "                profile = pd.concat([profile, serie.describe(percentiles=[.25, .5, .75, 1], datetime_is_numeric=True)])\n",
    "\n",
    "                if is_numeric_dtype(serie):\n",
    "                    profile[\"Negative\"] = (serie < 0).sum()\n",
    "                    profile[\"Negative (%)\"] = (\n",
    "                        str(round((serie < 0).sum() / len(serie) * 100, 2)) + \" %\"\n",
    "                    )\n",
    "                    profile[\"Zeros\"] = (serie == 0).sum()\n",
    "                    profile[\"Zeros (%)\"] = (\n",
    "                        str(round((serie == 0).sum() / len(serie) * 100, 2)) + \" %\"\n",
    "                    )\n",
    "                    only_outliers = serie.loc[(np.abs(ss.zscore(serie)) > 3)]\n",
    "                    profile[\"Outliers\"] = only_outliers.shape[0]\n",
    "                    profile[\"Outliers (%)\"] = (\n",
    "                        str(round(only_outliers.shape[0] / len(serie) * 100, 2)) + \" %\"\n",
    "                    )\n",
    "\n",
    "                profile[\"Missing cells\"] = serie.isnull().sum()\n",
    "                profile[\"Missing cells (%)\"] = (\n",
    "                    str(round(serie.isnull().sum() / len(serie) * 100, 2)) + \" %\"\n",
    "                )\n",
    "\n",
    "                profile = profile.rename(\n",
    "                    index={\n",
    "                        \"count\": \"Number of observations\",\n",
    "                        \"mean\": \"Mean\",\n",
    "                        \"std\": \"Std\",\n",
    "                        \"min\": \"Min\",\n",
    "                        \"max\": \"Max\",\n",
    "                        \"unique\": \"Unique\",\n",
    "                        \"top\": \"Top\",\n",
    "                        \"freq\": \"Freq\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Guardando la data a un solo string\n",
    "                txt += profile.to_string() + \"\\n\\n\"\n",
    "\n",
    "            # Pasando la data a un txt\n",
    "            new_file = open(f\"{self.eda_path}/summary.txt\", \"w\")\n",
    "            new_file.write(txt)\n",
    "            new_file.close()\n",
    "\n",
    "    def plot_vars(self, columns, N=10):\n",
    "        \"\"\"\n",
    "        Grafica la distribuci√≥n y correlaci√≥n de las variables. Se puede hacer de una variable o de varias.\n",
    "        Guarda los gr√°ficos en la carpeta EDA_fecha. En caso de que las variables sean categ√≥ricas, se hace\n",
    "        una matriz de correlaci√≥n de la V de Cramer. En caso de que sean num√©ricas, se hace una matriz de\n",
    "        correlaci√≥n de Pearson.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a graficar. Funciona como filtro.\n",
    "        N: int\n",
    "            N√∫mero de categor√≠as m√°s comunes a graficar. S√≥lo funciona para variables categ√≥ricas.\n",
    "        \"\"\"\n",
    "        # Viendo los tipos\n",
    "        assert type(columns) == str or type(columns) == list\n",
    "        assert type(N) == int\n",
    "\n",
    "        # Generando la carpeta de plots\n",
    "        plot_path = self.eda_path + \"/plots\"\n",
    "        if not os.path.exists(plot_path):\n",
    "            os.mkdir(plot_path)\n",
    "\n",
    "        if type(columns) == str:\n",
    "            serie_in = self.df[columns]\n",
    "            serie = serie_in.copy()\n",
    "\n",
    "            if is_numeric_dtype(serie):\n",
    "                ## Variables num√©ricas\n",
    "                # Gr√°fico de distribuci√≥n\n",
    "                fig = ff.create_distplot([list(serie.values)], [serie.name], show_hist=False, show_rug=False)\n",
    "                fig.update_layout(title_text=f\"Distribuci√≥n de {serie.name}\")\n",
    "                fig.write_image(plot_path+f\"/{serie.name}_dist.pdf\")\n",
    "\n",
    "                # Matriz de correlaci√≥n entre variables num√©ricas\n",
    "                corr = self.df.corr(numeric_only=True)\n",
    "                sns.heatmap(corr, annot=True).set(title=\"Matriz de correlaci√≥n de variables num√©ricas\")\n",
    "                plt.savefig(plot_path+\"/corrMatrix.pdf\")\n",
    "            else:\n",
    "                ## Variables categ√≥ricas\n",
    "                # Histograma de N categor√≠as m√°s comunes\n",
    "                count = (\n",
    "                    serie.value_counts()[0:N]\n",
    "                    .reset_index()\n",
    "                    .rename(columns = {serie.name: 'Count'})\n",
    "                )\n",
    "                fig = px.bar(\n",
    "                    x=count['index'].astype(str),\n",
    "                    y=count[\"Count\"],\n",
    "                    title=f\"{N} categor√≠as m√°s comunes {serie.name}\",\n",
    "                )\n",
    "                fig.write_image(plot_path+f\"/{serie.name}_nCategories.pdf\")\n",
    "\n",
    "                # Obteniendo la matriz de correlaci√≥n de la V de Cramer\n",
    "                num_cols=self.df.corr(numeric_only=True).columns\n",
    "                cols=[col for col in self.df.columns if not col in num_cols]\n",
    "                cols=[col for col in cols if \"-\" not in col]\n",
    "                corrM = np.zeros((len(cols),len(cols)))\n",
    "                for col1, col2 in itertools.combinations(cols, 2):\n",
    "                    idx1, idx2 = cols.index(col1), cols.index(col2)\n",
    "                    corrM[idx1, idx2] = cramers_corrected_stat(pd.crosstab(self.df[col1], self.df[col2]))\n",
    "                    corrM[idx2, idx1] = corrM[idx1, idx2]\n",
    "                corr = pd.DataFrame(corrM, index=cols, columns=cols)\n",
    "                sns.heatmap(corr, annot=True).set(title=\"Matriz de correlaci√≥n de variables categ√≥ricas\")\n",
    "                plt.savefig(plot_path+\"/cramerVCorrMap.pdf\")\n",
    "        else:\n",
    "            numeric = 0\n",
    "            categorical = 0\n",
    "            for c in columns:\n",
    "                serie_in = self.df[c]\n",
    "                serie = serie_in.copy()\n",
    "\n",
    "                if is_numeric_dtype(serie):\n",
    "                    numeric == 1\n",
    "                    ## Variables num√©ricas\n",
    "                    # Gr√°fico de distribuci√≥n\n",
    "                    fig = ff.create_distplot([list(serie.values)], [serie.name], show_hist=False, show_rug=False)\n",
    "                    fig.update_layout(title_text=f\"Distribuci√≥n de {serie.name}\")\n",
    "                    fig.write_image(plot_path+f\"/{serie.name}_dist.pdf\")\n",
    "                else:\n",
    "                    categorical == 1\n",
    "                    ## Variables categ√≥ricas\n",
    "                    # Histograma de N categor√≠as m√°s comunes\n",
    "                    count = (\n",
    "                        serie.value_counts()[0:N]\n",
    "                        .reset_index()\n",
    "                        .rename(columns = {serie.name: 'Count'})\n",
    "                    )\n",
    "                    fig = px.bar(\n",
    "                        x=count['index'].astype(str),\n",
    "                        y=count[\"Count\"],\n",
    "                        title=f\"{N} categor√≠as m√°s comunes {serie.name}\",\n",
    "                    )\n",
    "                    fig.write_image(plot_path+f\"/{serie.name}_nCategories.pdf\")\n",
    "\n",
    "            # S√≥lo son necesarios un gr√°fico de correlaci√≥n num√©rica y/o categ√≥rica,\n",
    "            # seg√∫n corresponda\n",
    "            if numeric == 1:\n",
    "                # Matriz de correlaci√≥n entre variables num√©ricas\n",
    "                corr = self.df.corr(numeric_only=True)\n",
    "                sns.heatmap(corr, annot=True).set(title=\"Matriz de correlaci√≥n de variables num√©ricas\")\n",
    "                plt.savefig(plot_path+\"/corrMatrix.pdf\")\n",
    "            if categorical == 1:\n",
    "                # Obteniendo la matriz de correlaci√≥n de la V de Cramer\n",
    "                num_cols=self.df.corr(numeric_only=True).columns\n",
    "                cols=[col for col in self.df.columns if not col in num_cols]\n",
    "                cols=[col for col in cols if \"-\" not in col]\n",
    "                corrM = np.zeros((len(cols),len(cols)))\n",
    "                for col1, col2 in itertools.combinations(cols, 2):\n",
    "                    idx1, idx2 = cols.index(col1), cols.index(col2)\n",
    "                    corrM[idx1, idx2] = cramers_corrected_stat(pd.crosstab(self.df[col1], self.df[col2]))\n",
    "                    corrM[idx2, idx1] = corrM[idx1, idx2]\n",
    "                corr = pd.DataFrame(corrM, index=cols, columns=cols)\n",
    "                sns.heatmap(corr, annot=True).set(title=\"Matriz de correlaci√≥n de variables categ√≥ricas\")\n",
    "                plt.savefig(plot_path+\"/cramerVCorrMap.pdf\")\n",
    "\n",
    "    def clean_data(self, columns, nonatomic, drop_na=True):\n",
    "        \"\"\"\n",
    "        Limpia los datos. Se puede hacer de una variable o de varias. Se pueden eliminar los registros con\n",
    "        valores nulos o imputar valores. Guarda el dataframe limpio en la carpeta EDA_fecha.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a limpiar\n",
    "        nonatomic: list\n",
    "            Lista de columnas no at√≥micas a separar\n",
    "        drop_na: bool\n",
    "            Si es True, se eliminan los registros con valores nulos. Si es False, se imputan el valor m√°s frecuente\n",
    "            de la columna.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        serie: DataFrame\n",
    "            DataFrame limpio\n",
    "        \"\"\"\n",
    "        # Viendo los tipos\n",
    "        assert type(columns) == list or type(columns) == str\n",
    "        assert type(drop_na) == bool\n",
    "        assert type(nonatomic) == list\n",
    "\n",
    "        # Generando la carpeta de clean_data\n",
    "        clean_path = self.eda_path + \"/clean_data\"\n",
    "        if not os.path.exists(clean_path):\n",
    "            os.mkdir(clean_path)\n",
    "\n",
    "        # Limpiando los datos\n",
    "        serie = self.df.copy()\n",
    "        serie = serie[columns]\n",
    "\n",
    "        pipeline = Pipeline([('Eliminar duplicados', FunctionTransformer(lambda x: x.drop_duplicates())), \n",
    "                    ('Separar datos no at√≥micos', FunctionTransformer(lambda x: separate_columns(x, nonatomic)[0])),\n",
    "                    ('Imputar valores nulos', FunctionTransformer(lambda x: x.dropna()) if drop_na else SimpleImputer(strategy='most_frequent'))\n",
    "                    ])\n",
    "        #Aplicando el pipeline\n",
    "        serie = pipeline.fit_transform(serie)\n",
    "        serie = pd.DataFrame(serie, columns=serie.columns).reset_index(drop=True)\n",
    "\n",
    "        # Guardando el dataframe limpio\n",
    "        serie.to_csv(clean_path+\"/data.csv\")\n",
    "        return serie\n",
    "    \n",
    "    def scale(self, *args, columns, nonatomic, drop_na=True):\n",
    "        \"\"\"\n",
    "        Escala los datos. Se puede hacer de una variable o de varias. Una vez limpio el dataframe, se \n",
    "        ingresan los m√©todos de escalamiento que se quieran aplicar, que sean compatibles con\n",
    "        sklearn. Guarda el dataframe escalado en la carpeta EDA_fecha.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        args: list\n",
    "            Lista de m√©todos de escalamiento a aplicar. Deben ser compatibles con sklearn.\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a escalar\n",
    "        nonatomic: list\n",
    "            Lista de columnas no at√≥micas a separar\n",
    "        drop_na: bool\n",
    "            Si es True, se eliminan los registros con valores nulos. Si es False, se imputan el valor m√°s frecuente\n",
    "            de la columna.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ctrans: ColumnTransformer\n",
    "            ColumnTransformer con los m√©todos de escalamiento aplicados\n",
    "        \"\"\"\n",
    "\n",
    "        # Viendo los tipos\n",
    "        assert type(columns) == list or type(columns) == str\n",
    "        assert type(drop_na) == bool\n",
    "        assert type(nonatomic) == list\n",
    "\n",
    "        # Viendo los tipos y asegurando que los m√©todos existan en el modulo de sklearn\n",
    "        for method in args:\n",
    "            assert hasattr(sklearn.preprocessing, method.__class__.__name__)\n",
    "            assert method.__class__.__name__ in sklearn.preprocessing.__all__, 'El m√©todo no es compatible con framework sklearn'\n",
    "            \n",
    "        # Generando la carpeta de scale\n",
    "        scale_path = self.eda_path + \"/scale\"\n",
    "        if not os.path.exists(scale_path):\n",
    "            os.mkdir(scale_path)\n",
    "        \n",
    "        # Copiando los datos\n",
    "        serie = self.clean_data(columns, nonatomic, drop_na)\n",
    "        num_cols = serie.corr(numeric_only=True).columns\n",
    "        cat_cols = [col for col in serie.columns if not col in num_cols]\n",
    "\n",
    "        # Atributos num√©ricos\n",
    "        num_pipe = Pipeline([\n",
    "                        ('Logaritmic scaler', FunctionTransformer(lambda x: np.log(x + 1), feature_names_out='one-to-one')),\n",
    "                        ('MinMax scaler', MinMaxScaler())\n",
    "                    ])\n",
    "        # Atributos categ√≥ricos\n",
    "        cat_pipe = Pipeline(steps = [ (f\"Scaling method {i}\", args[i]) for i in range(len(args)) ])\n",
    "\n",
    "        # Creando ColumnTransformer\n",
    "        ctrans = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"Categorico\", cat_pipe, cat_cols),\n",
    "                (\"Numerico\", num_pipe, num_cols),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Aplicando la pipeline\n",
    "        serie2 = ctrans.fit_transform(serie)\n",
    "        display(serie2)\n",
    "\n",
    "        # Creando el dataframe final\n",
    "        seriefinal = pd.DataFrame(serie2.toarray(), columns=ctrans.fit(serie).get_feature_names_out()).reset_index(drop=True)  \n",
    "\n",
    "        # Guardando el dataframe limpio\n",
    "        seriefinal.to_csv(scale_path+\"/scaled_features.csv\")\n",
    "\n",
    "        # Se retorna s√≥lo el ColumnTransformer para las futuras pipelines\n",
    "        return ctrans\n",
    "\n",
    "    def make_clusters(self, clustering, k, threshold, *args, columns, nonatomic, drop_na=True):\n",
    "        \"\"\"\n",
    "        Genera clusters de los datos usando alg√∫n m√©todo de clustering de sklearn. Se puede hacer de una variable o de varias.\n",
    "        El dataframe se limpia y escala autom√°ticamente. Guarda el dataframe con los clusters en la carpeta EDA_fecha.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clustering: Clustering method\n",
    "            M√©todo de clustering de sklearn\n",
    "        k: int\n",
    "            N√∫mero m√°ximo de clusters a generar\n",
    "        threshold: int\n",
    "            Umbral para el m√©todo del codo\n",
    "        args: list\n",
    "            Lista de m√©todos de escalamiento a aplicar. Deben ser compatibles con sklearn.\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a escalar\n",
    "        nonatomic: list\n",
    "            Lista de columnas no at√≥micas a separar\n",
    "        drop_na: bool\n",
    "            Si es True, se eliminan los registros con valores nulos. Si es False, se imputan el valor m√°s frecuente\n",
    "            de la columna.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Viendo los tipos\n",
    "        assert type(columns) == list or type(columns) == str\n",
    "        assert type(drop_na) == bool\n",
    "        assert type(nonatomic) == list\n",
    "        assert type(k) == int\n",
    "        assert type(threshold) == int\n",
    "        assert hasattr(sklearn.cluster, clustering.__class__.__name__)\n",
    "        assert clustering.__class__.__name__ in sklearn.cluster.__all__\n",
    "\n",
    "        # Viendo los tipos y asegurando que los m√©todos existan en el modulo de sklearn\n",
    "        for method in args:\n",
    "            assert hasattr(sklearn.preprocessing, method.__class__.__name__)\n",
    "            assert method.__class__.__name__ in sklearn.preprocessing.__all__, 'El m√©todo no es compatible con framework sklearn'\n",
    "\n",
    "        # Generando la carpeta de clusters\n",
    "        cluster_path = self.eda_path + \"/clusters\"\n",
    "        if not os.path.exists(cluster_path):\n",
    "            os.mkdir(cluster_path)\n",
    "\n",
    "        # Limpiando la data\n",
    "        serie = self.clean_data(columns, nonatomic, drop_na)\n",
    "\n",
    "        # Obteniendo el ColumnTransformer\n",
    "        ctrans = self.scale(*args, columns=columns, nonatomic=nonatomic, drop_na=drop_na)\n",
    "            \n",
    "        # Estudio del codo\n",
    "        inertias = []\n",
    "        for c in range(1,k):\n",
    "            pipeline = Pipeline([\n",
    "                (\"Scaling\", ctrans),\n",
    "                (\"Clustering\", clustering.__class__(n_clusters=c, n_init=\"auto\"))\n",
    "            ])\n",
    "                \n",
    "            if len(inertias) == 0:\n",
    "                inertias = [c, pipeline.fit(serie)[\"Clustering\"].inertia_]\n",
    "\n",
    "            else:                \n",
    "                new = pipeline.fit(serie)[\"Clustering\"].inertia_\n",
    "                old = inertias[1]\n",
    "                if old - new < threshold:\n",
    "                    inertias = [c, new]\n",
    "                    break\n",
    "                else:\n",
    "                    inertias = [c, new]\n",
    "\n",
    "        # Definiendo los clusters\n",
    "        pipeline = Pipeline([\n",
    "                (\"Scaling\", ctrans),\n",
    "                (\"Clustering\", clustering.__class__(n_clusters=inertias[0], n_init=\"auto\"))\n",
    "        ])\n",
    "        new_k = pipeline.fit(serie)\n",
    "\n",
    "        # Representaci√≥n 2D\n",
    "        pipe_scaled = Pipeline([\n",
    "            (\"Scaling\", ctrans),\n",
    "            (\"2D representation\", TruncatedSVD(n_components=2))\n",
    "        ])\n",
    "        new = pipe_scaled.fit_transform(serie)\n",
    "\n",
    "        # Agreg√°ndolos al dataframe\n",
    "        serie[\"Cluster\"] = new_k[\"Clustering\"].labels_\n",
    "        serie.to_csv(cluster_path+\"/data_clusters.csv\")\n",
    "\n",
    "        # Gr√°fico representaci√≥n de dimensionalidad reducida\n",
    "        fig = px.scatter(\n",
    "            new, x=0, y=1, title=\"Representaci√≥n 2D de los datos y sus clusters\", color=new_k[\"Clustering\"].labels_, height=800, width=800\n",
    "        )\n",
    "        fig.write_image(cluster_path+\"/clusters.pdf\")\n",
    "\n",
    "    def detect_anomalies(self, detector, *args, columns, nonatomic, drop_na=True):\n",
    "        \"\"\"\n",
    "        Detecta anomal√≠as en los datos usando alguna t√©cnica de detecci√≥n de anomal√≠as de sklearn. Se puede hacer de una variable o de varias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        detector: Detector method\n",
    "            M√©todo de detecci√≥n de anomal√≠as de sklearn\n",
    "        args: list\n",
    "            Lista de m√©todos de escalamiento a aplicar. Deben ser compatibles con sklearn.\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a escalar\n",
    "        nonatomic: list\n",
    "            Lista de columnas no at√≥micas a separar\n",
    "        drop_na: bool\n",
    "            Si es True, se eliminan los registros con valores nulos. Si es False, se imputan el valor m√°s frecuente\n",
    "            de la columna.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Viendo los tipos\n",
    "        assert type(columns) == list or type(columns) == str\n",
    "        assert type(drop_na) == bool\n",
    "        assert type(nonatomic) == list\n",
    "        assert hasattr(sklearn.ensemble, detector.__class__.__name__)\n",
    "        assert detector.__class__.__name__ in sklearn.ensemble.__all__\n",
    "\n",
    "        # Viendo los tipos y asegurando que los m√©todos existan en el modulo de sklearn\n",
    "        for method in args:\n",
    "            assert hasattr(sklearn.preprocessing, method.__class__.__name__)\n",
    "            assert method.__class__.__name__ in sklearn.preprocessing.__all__, 'El m√©todo no es compatible con framework sklearn'\n",
    "\n",
    "        # Generando la carpeta de anomal√≠as\n",
    "        anomaly_path = self.eda_path + \"/anomalies\"\n",
    "        if not os.path.exists(anomaly_path):\n",
    "            os.mkdir(anomaly_path)\n",
    "\n",
    "        # Limpiando la data\n",
    "        serie = self.clean_data(columns, nonatomic, drop_na)\n",
    "\n",
    "        # Obteniendo el ColumnTransformer\n",
    "        ctrans = self.scale(*args, columns=columns, nonatomic=nonatomic, drop_na=drop_na)\n",
    "\n",
    "        # Obteniendo las anomal√≠as\n",
    "        pipeline = Pipeline([\n",
    "            (\"Scaling\", ctrans),\n",
    "            (\"Anomalies\", detector)\n",
    "        ])\n",
    "        outliers = pipeline.fit_predict(serie)\n",
    "\n",
    "        # Representaci√≥n 2D\n",
    "        pipe_scaled = Pipeline([\n",
    "            (\"Scaling\", ctrans),\n",
    "            (\"2D representation\", TruncatedSVD(n_components=2))\n",
    "        ])\n",
    "        new = pipe_scaled.fit_transform(serie)\n",
    "\n",
    "        # Agreg√°ndolos al dataframe\n",
    "        serie[\"Anomalies\"] = outliers\n",
    "        serie.to_csv(anomaly_path+\"/data_anomalies.csv\")\n",
    "\n",
    "        # Gr√°fico representaci√≥n de dimensionalidad reducida\n",
    "        fig = px.scatter(\n",
    "            new, x=0, y=1, title=\"Representaci√≥n 2D de los datos y sus anomal√≠as\", color=outliers, height=800, width=800\n",
    "        )\n",
    "        fig.write_image(anomaly_path+\"/anomalies.pdf\")\n",
    "\n",
    "    def profile(self, columns, anomalies, N, nonatomic, drop_na, clustering, k, threshold, detector, *args):\n",
    "        \"\"\"\n",
    "        Funci√≥n que permite hacer un an√°lisis exploratorio de datos (EDA) de un dataframe, ejecutando todos los m√©todos.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns: str or list\n",
    "            Nombre de la(s) variable(s) a caracterizar\n",
    "        anomalies: list\n",
    "            Lista de anomal√≠as a revisar. Actualmente s√≥lo se puede revisar el formato de las variables\n",
    "        N: int\n",
    "            N√∫mero de categor√≠as m√°s comunes a graficar. S√≥lo funciona para variables categ√≥ricas.\n",
    "        nonatomic: list\n",
    "            Lista de columnas no at√≥micas a separar\n",
    "        drop_na: bool\n",
    "            Si es True, se eliminan los registros con valores nulos. Si es False, se imputan el valor m√°s frecuente\n",
    "            de la columna.\n",
    "        clustering: Clustering method\n",
    "            M√©todo de clustering de sklearn\n",
    "        k: int\n",
    "            N√∫mero m√°ximo de clusters a generar\n",
    "        threshold: int\n",
    "            Umbral para el m√©todo del codo\n",
    "        detector: Detector method\n",
    "            M√©todo de detecci√≥n de anomal√≠as de sklearn\n",
    "        args: list\n",
    "            Lista de m√©todos de escalamiento a aplicar. Deben ser compatibles con sklearn.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Haciendo el resumen\n",
    "        self.summarize(columns, anomalies)\n",
    "\n",
    "        # Haciendo los gr√°ficos de las variables\n",
    "        self.plot_vars(columns, N)\n",
    "\n",
    "        # Al hacer los clusters, se hacen de forma autom√°tica la limpieza y el escalamiento\n",
    "        self.make_clusters(clustering, k, threshold, *args, columns=columns, nonatomic=nonatomic, drop_na=drop_na)\n",
    "\n",
    "        # Detectando anomal√≠as (que tambi√©n hace internamente la limpieza y el escalamiento)\n",
    "        self.detect_anomalies(detector, *args, columns=columns, nonatomic=nonatomic, drop_na=drop_na)\n",
    "\n",
    "    def clearGarbage(self):\n",
    "        \"\"\"\n",
    "        Elimina la carpeta del d√≠a si es que existe. Si no existe, no hace nada.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Si es que existe la carpeta del d√≠a, eliminarla\n",
    "        if os.path.exists(self.eda_path):\n",
    "            os.rmdir(self.eda_path)\n",
    "            print(\"Carpeta eliminada exitosamente\")\n",
    "        print(\"Carpeta no encontrada / Ya eliminada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sanfe\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pds-py (c:\\users\\sanfe\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se requiere hacer una caracterizaci√≥n autom√°tica y eficiente de los Juegos Panamericanos Santiago 2023, con tal de poder hacer un an√°lisis respecto a estos para poder comprender de mejor manera el impacto a nivel pa√≠s que puedan tener. Adem√°s, dado el √©xito que han tenido y la posibilidad de ser anfitriones de unos futuros Juegos Ol√≠mpicos, es importante caracterizar los distintos datos de estos para poder organizar y preparar de mejor manera otro evento de esta envergadura.\n",
    "\n",
    "Dado todo lo anterior, se espera generar un an√°lisis exploratorio de los datos entregados, pero buscando ser lo m√°s generales posibles. As√≠, en el presente informe, se espera lograr dicha exploraci√≥n/caracterizaci√≥n de datos de forma autom√°tica, la cual es compatible con cualquier dataset. En particular, para probar dicha forma se ha facilitado el dataset *olimpiadas.parquet*, en donde se tiene informaci√≥n de distintos JJOO recientes. Este dataset es de gran utilidad puesto que permite entender c√≥mo se definen los sets de datos de eventos deportivos masivos como estos, por lo que es una aplicaci√≥n real a lo que se quiere hacer finalmente con los Juegos Panamericanos. \n",
    "\n",
    "En cuanto al resumen del dataset crudo, este consta de 271116 filas y 13 columnas. Las columnas son las siguientes:\n",
    "\n",
    "- ID: Identificador √∫nico de cada atleta. Es un n√∫mero entero asignado secuencialmente. Existen 135571 valores √∫nicos en esta columna.\n",
    "- Name (nombre): Nombre del atleta. Variable categ√≥rica transformable a cadena de texto. A diferencia del ID, existen 134732 valores √∫nicos.\n",
    "- Sex (sexo): Sexo del atleta. Variable categ√≥rica transformable a cadena de texto. Existen 2 valores √∫nicos en esta columna: 'M' y 'F'.\n",
    "- Team (equipo): Equipo/naci√≥n a la que pertenece el atleta. Variable categ√≥rica transformable a cadena de texto. Existen 1184 valores √∫nicos en esta columna.\n",
    "- NOC (C√≥digo del Comit√© Ol√≠mpico Nacional): C√≥digo del Comit√© Ol√≠mpico Nacional, asociado al equipo. Variable categ√≥rica transformable a cadena de texto. Existen 230 valores √∫nicos en esta columna. Estados Unidos es el valor m√°s frecuente.\n",
    "- Games (juegos): A√±o y temporada de los juegos ol√≠mpicos. Variable categ√≥rica transformable a cadena de texto. Existen 51 valores √∫nicos en esta columna.\n",
    "- Year (a√±o): A√±o de los juegos ol√≠mpicos. Variable num√©rica. Existen 35 valores √∫nicos en esta columna, desde 1896 hasta 2016.\n",
    "- Season (temporada): Temporada de los juegos ol√≠mpicos. Variable categ√≥rica transformable a cadena de texto. Existen 2 valores √∫nicos en esta columna: 'Summer' y 'Winter'.\n",
    "- City (ciudad): Ciudad donde se realizaron los juegos ol√≠mpicos. Variable categ√≥rica transformable a cadena de texto. Existen 42 valores √∫nicos en esta columna.\n",
    "- Sport (deporte): Deporte en el que particip√≥ el atleta en alguna temporada en particular. Variable categ√≥rica transformable a cadena de texto. Existen 66 valores √∫nicos en esta columna. El registro m√°s frecuente es 'Athletics'.\n",
    "- Event (evento): Nombre del eventro deportivo en el que particip√≥ el atleta en alguna temporada en particular. Variable categ√≥rica transformable a cadena de texto. Existen 765 valores √∫nicos en esta columna, siendo f√∫tbol masculino el m√°s frecuente.\n",
    "- Medal (medalla): Medalla obtenida por el atleta en alg√∫n evento deportivo en particular. Variable categ√≥rica transformable a cadena de texto. Existen 4 valores √∫nicos en esta columna: 'Gold', 'Silver', 'Bronze' y 'NA'. El valor m√°s frecuente es 'NA', que corresponde a los atletas que no obtuvieron medalla o de los que no se tiene registro.\n",
    "- age-height-weight (edad-altura-peso): Variable categ√≥rica no at√≥mica separable en columnas num√©ricas. Contiene la edad, altura y peso del atleta en el momento de participar en un evento deportivo. Existen 82971 valores √∫nicos en esta columna, siendo la mayoria de ellos valores nulos para los 3 atributos.\n",
    "\n",
    "Para poder hacer el an√°lisis de este dataset y, por ende, el futuro an√°lisis de los Juegos Panamericanos Santiago 2023, se crea la clase `Profiler`. Esta clase cumple todos los requisitos anteriores, siendo aplicable a cualquier dataset y capaz de caracterizar autom√°ticamente, tal como requiera el usuario, todos los datos entregados. Esto lo hace a trav√©s de procesos de exploraci√≥n, limpieza, escalamiento y clusterizaci√≥n de los datos, al igual que la detecci√≥n de distintas anomal√≠as en ellos. De esta forma, se espera que dicha clase creada sea capaz de llevar a cabo todas las indicaciones pedidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis del EDA (An√°lisis Exploratorio de Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Anomal√≠a detectada en columna Name: caracteres extra√±os",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanfe\\Documents\\M√≠o\\Trabajos\\Universidad\\El√©ctrica\\Semestre 7\\Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\MDS7202-BS\\Proyecto1\\Proyecto1_Becerra_Sanfeli√∫.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m prof \u001b[39m=\u001b[39m Profiler(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Obteniendo res√∫menes para cada columnas\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m prof\u001b[39m.\u001b[39;49msummarize(df\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mtolist(), [\u001b[39m\"\u001b[39;49m\u001b[39mformat\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Obteniendo las categor√≠as de las variables categ√≥ricas\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m prof\u001b[39m.\u001b[39mplot_vars(cat_cols, \u001b[39m10\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\sanfe\\Documents\\M√≠o\\Trabajos\\Universidad\\El√©ctrica\\Semestre 7\\Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\\MDS7202-BS\\Proyecto1\\Proyecto1_Becerra_Sanfeli√∫.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m             check \u001b[39m=\u001b[39m serie\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39moutlier\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(a \u001b[39min\u001b[39;00m x \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m chars) \u001b[39melse\u001b[39;00m x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moutlier\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m check\u001b[39m.\u001b[39mvalues:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAnomal√≠a detectada en columna \u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m: caracteres extra√±os\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m profile \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanfe/Documents/M%C3%ADo/Trabajos/Universidad/El%C3%A9ctrica/Semestre%207/Laboratorio%20de%20Programaci%C3%B3n%20Cient%C3%ADfica%20para%20Ciencia%20de%20Datos/MDS7202-BS/Proyecto1/Proyecto1_Becerra_Sanfeli%C3%BA.ipynb#X24sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m profile[\u001b[39m\"\u001b[39m\u001b[39mName\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m c\n",
      "\u001b[1;31mException\u001b[0m: Anomal√≠a detectada en columna Name: caracteres extra√±os"
     ]
    }
   ],
   "source": [
    "# Leyendo el dataset\n",
    "df = pd.read_parquet(\"olimpiadas.parquet\")\n",
    "\n",
    "# Separando en columnas num√©ricas y categ√≥ricas\n",
    "num_cols = df.corr(numeric_only=True).columns\n",
    "cat_cols = [col for col in df.columns if not col in num_cols]\n",
    "\n",
    "# Generando la clase\n",
    "prof = Profiler(df)\n",
    "\n",
    "# Obteniendo res√∫menes para cada columnas\n",
    "prof.summarize(df.columns.tolist(), [\"format\"])\n",
    "\n",
    "# Obteniendo las categor√≠as de las variables categ√≥ricas\n",
    "prof.plot_vars(cat_cols, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leyendo el dataset\n",
    "df = pd.read_parquet(\"olimpiadas.parquet\")\n",
    "\n",
    "# Separando en columnas num√©ricas y categ√≥ricas\n",
    "num_cols = df.corr(numeric_only=True).columns\n",
    "cat_cols = [col for col in df.columns if not col in num_cols]\n",
    "\n",
    "# Generando la clase\n",
    "prof = Profiler(df)\n",
    "\n",
    "# Obteniendo res√∫menes para cada columnas\n",
    "prof.summarize(df.columns.tolist(), [])\n",
    "\n",
    "# Obteniendo las categor√≠as de las variables categ√≥ricas\n",
    "prof.plot_vars(cat_cols, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia, es posible ver que se logran encontrar columnas con anomal√≠as en la forma en la que est√°n entregados ciertos textos. Por ejemplo, se logr√≥ detectar que en la columna de nombres contiene caracteres no deseados, por lo que es relevante revisar estos datos para darles una limpieza especial seg√∫n se requiera. \n",
    "\n",
    "Con el "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci√≥n de Clusters y Anomal√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30141x888 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 241111 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<30141x888 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 241111 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generando la clase\n",
    "prof = Profiler(df)\n",
    "\n",
    "# Creaci√≥n de clusters usando columnas m√°s relevantes\n",
    "cols = ['Sex', 'Team', 'Sport', 'Event', 'Medal', 'age-height-weight']\n",
    "\n",
    "# Clusterizando\n",
    "prof.make_clusters(KMeans(n_init='auto'), 10, 15, OneHotEncoder(), columns=cols, nonatomic=['age-height-weight'], drop_na=True)\n",
    "\n",
    "# Detectando anomal√≠as\n",
    "prof.detect_anomalies(IsolationForest(), OneHotEncoder(), columns=cols, nonatomic=['age-height-weight'], drop_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
